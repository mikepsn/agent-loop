\documentclass[aspectratio=1610,xcolor=dvipsnames,t]{beamer} 

\usepackage{listings} 
\usepackage{color} 
\usepackage{xcolor}  
\usepackage{microtype} 
\usepackage{helvet} 
\usepackage{inconsolata} 
\usepackage[framemethod=TikZ]{mdframed} 
\usepackage{graphicx} 
\usepackage{alltt}
\usepackage{sverb} 
\usepackage{verbatim} 
\usepackage{pifont} 
\usepackage{alltt} 
\usepackage{helvet} 
\usepackage{algorithm}
\usepackage{algpseudocode}

\usetheme{Madrid} 
%\useoutertheme{smoothbars} 
\useinnertheme{rectangles} 

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{blocks}[default] 

%\definecolor{mypurple}{rgb}{.49,0,98}
%\setbeamercolor*{palette primary}{use=structure,fg=white,bg=green}
%\usecolortheme[rgb={0.9,0.2,0.2}]{structure}
%\usecolortheme[rgb={0.6,0.1,0.1}]{structure}

%\usecolortheme[rgb={0.2, 0.2, 0.8}]{structure} 
\usecolortheme[rgb={0.0, 0.0, 0.8}]{structure} 

\usepackage{color}
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
\definecolor{myteal}{rgb}{.26, .44, .56}
\definecolor{gray}{gray}{0.45}
\definecolor{lightgray}{gray}{.95}
\definecolor{mediumgray}{gray}{.8}
\definecolor{inputbackground}{rgb}{.95, .95, .85}
\definecolor{outputbackground}{rgb}{.95, .95, .95}
\definecolor{traceback}{rgb}{1, .95, .95}
\definecolor{inputbg}{rgb}{0.98, 0.98, 0.98}

\usepackage{listings} 
\lstset{language=c,
        %basicstyle=\footnotesize\ttfamily, 
        basicstyle=\small\ttfamily,
        columns=fullflexible, 
        %title=\lstname, 
        %numbers=left, stringstyle=\texttt, 
        %numberstyle={\tiny\texttt}, 
        keywordstyle=\color{blue}, 
        commentstyle=\color{darkgreen}, 
        stringstyle=\color{purple} } 


\mdfsetup{skipabove=\topskip, skipbelow=\topskip} 

\definecolor{codebg}{rgb}{0.99,0.99,0.99}

\global\mdfdefinestyle{code}{%
    frametitlerule=true,%
    frametitlefont=\small\bfseries\ttfamily,%
    frametitlebackgroundcolor=lightgray,%
    backgroundcolor=codebg,%
    linecolor=gray, linewidth=0.5pt,%
    leftmargin=0.5cm, rightmargin=0.5cm,%
    roundcorner=2pt,%
    innerleftmargin=5pt
}

\global\mdfdefinestyle{code2}{%
    topline=false,%
    bottomline=false,%
    leftline=true,%
    rightline=false,%
    backgroundcolor=codebg,%
    linecolor=gray, linewidth=0.5pt,%
    leftmargin=0.0cm, rightmargin=0.0cm,%
    innerleftmargin=1pt
}

\newcommand{\showcode}[1]{\begin{mdframed}[style=code] %
                            \lstinputlisting{#1}% 
                          \end{mdframed}% 
}


\title[Intelligent Agents]{Rethinking the Intelligent Agent \\ Perceive-Reason-Act Loop} 
%\subtitle{Intelligent Agent Lab} 
\author[Michael Papasimeon]{Michael Papasimeon \\[0.2cm] \tiny{Intelligent Agent Lab} } 
\date{30 October 2002} 

\begin{document}

\begin{frame}
    \maketitle
\end{frame} 

\begin{frame}{Agent-Environment Interaction} 
    Key issues with current approaches to agent-environment interaction:
    \begin{itemize}
        \item Treat the agent and the environment as separate entities.
        \item Communication via inputs and outputs.
        \item Agent-Environment designs do not follow claims about:
            \begin{itemize}
                \item Agents being situated.
                \item The environment being important. 
            \end{itemize} 
    \end{itemize} 
\end{frame} 

\begin{frame}{Agent-Environment Interaction Loop} 
    \begin{center}
        \includegraphics[width=0.8\textwidth]{loop} 
    \end{center} 
\end{frame} 

\begin{frame}{Agent Control Loop...}
    \begin{block}{Pythonic Version of Wooldridge's Agent Control Loop} 
        \showcode{wooldridge.py} 
    \end{block} 
\end{frame} 

\begin{frame}{Or the BDI Control Loop...}
    \begin{block}{Adapted from Wooldridge...} 
        \begin{algorithmic}[0] 
            \Procedure{BDI}{$B_0, I_0$} 
                \State $B \gets B_0$
                \State $I \gets I_0$
                \While{True} 
                    \State $\rho \gets \texttt{get\_next\_percept}();$
                    \State $B \gets \texttt{brf}(B, \rho);$
                    \State $D \gets \texttt{options}(B, I);$ 
                    \State $I \gets \texttt{filter}(B, D, I); $
                    \State $\pi \gets \texttt{plan}(B, I); $
                    \State $\texttt{execute}(\pi); $
                \EndWhile
            \EndProcedure
        \end{algorithmic} 
    \end{block} 
\end{frame} 

\begin{frame}{Let's dig deeper...}
    \begin{itemize}
        \item Begin to look at the agent control loop and the interaction
              with the environment in more detail.
        \item The interaction between agent and environment needs to be
              broken down into components, step by step.
        \item Start looking at how inputs/outputs are generated...
              i.e. look at sensors and actuators.
    \end{itemize} 
\end{frame} 

\begin{frame}{A level down...}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{sensorsactuators} 
    \end{center} 
\end{frame} 

\begin{frame}{Labels in the Environment}  
    \begin{itemize} 
        \item One of the things that is sent to an agent's sensors
              is the possibility of pre-labeled entities in the 
              environment. 
    \end{itemize} 
\end{frame} 

\begin{frame}{We can begin to formulate a theory...}
    \begin{itemize}
        \item In a multi-agent system we have $n$ agents, $A_1 ... A_n$. 
        \item Each agent has $m$ sensors.
        \item We can specify the $i$-th agent's $j$-th sensor as $S_{ij}$
    \end{itemize} 
\end{frame} 

\begin{frame}{Agent Mental States}
    \begin{itemize} 
        \item Each agent $A_i$ can be in a single mental state $m_i$. 
        \item The mental state may be the agent's beliefs and intentions.
        \item $m_i = \{B_i, I_i\}$
        \item Consider the sensing of the environment to be a function of the
              agent's current mental state. 
    \end{itemize} 
\end{frame} 

\begin{frame}{Agent Mental State in the Loop...}
    \begin{center} 
        \includegraphics[width=0.7\textwidth]{mentalstateinloop} 
    \end{center} 
\end{frame} 

\begin{frame}{Perception and Mental State}
    \begin{itemize} 
        \item Implies perception/sensing are a function of an agent's mental
            state.
        \item What you perceive as an agent depends on what you are doing and
            what you believe you are doing (beliefs, intentions).
        \item This fits in with J.J. Gibsons ideas of direct perception for 
            ecological psychology. 
        \item $\texttt{Sensor}(\sigma_i, e, m_i) := \sigma_{i + 1}$ 
    \end{itemize} 
\end{frame} 

\begin{frame}{The Agent-Environment Loop Revisited}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{agent-loop-revisited} 
    \end{center} 
\end{frame} 

\begin{frame}{Intention Based Feedback Loop}
    \begin{center} 
        \includegraphics[width=0.5\textwidth]{intention-feedback} 
    \end{center} 
\end{frame} 

\begin{frame}{Environmental Representation} 
    \begin{itemize} 
        \item Still need to look at environmental representation options
            \begin{itemize}
                \item Flat
                \item Hierarchical/Relational
                \item Labels (Dynamic or Pre-Processed)
                \item Intention Oriented Affordances
            \end{itemize} 
        \item Dynamic agent (tailored to the agent)
        \item Static environment
    \end{itemize} 
\end{frame} 

\begin{frame}{So what is the goal then?}
    \begin{itemize} 
        \item To create a \emph{truly} situated agent.
        \item Affordances: opportunity for action
        \item Together with a tighter agent-environment feedback loop; might
            just do the trick.
    \end{itemize} 
\end{frame} 

\begin{frame}{Affordances}
    \begin{itemize} 
        \item Affordances are a function:
            \begin{itemize}
                \item The subset of the environment that the agent is/can
                    perceive using its sensors.
                \item The agent's mental state.
                \item The agent's current activity...
            \end{itemize} 
        \item Do we need to distinguish between Intention/Activity/Action?
    \end{itemize} 
\end{frame} 

\begin{frame}{Example: Jumping a Creek} 
    \begin{itemize} 
        \item My intention is to get to Town B from Town A
        \item I have a plan to \emph{run} from A to B
        \item I have a plan to \emph{walk} from A to B
        \item I see a creek
        \item If I am running the creek \emph{affords} jumping
        \item Here the affordance is a function of the activity rather than
            intention.
    \end{itemize} 
\end{frame} 

\begin{frame}{How do we build such an agent?}
    \begin{itemize} 
        \item Agent \emph{announces} to the environment what it can see.
        \item Agent \emph{announces} to th environment what it is doing
            (activity or action) or maybe even intention.
        \item Environment/Affordance engine somehow binds what a I can see with
            what I am doing, generating affordances for the things in the
            environment.
    \end{itemize} 
\end{frame} 

\begin{frame}{Issues (1)}
    \begin{itemize} 
        \item How does the agent sense/perceive the affordances?
        \item Is there an affordance \emph{sensor}?
        \item Does the agent get affordance percepts (direct percepts) in
            addition to regular percepts?
        \item How does the agent then use these affordances in the next
            deliberation step?
    \end{itemize} 
\end{frame} 

\begin{frame}{Issues (2)}
    \begin{itemize} 
        \item What do affordances look like? \\ Names, labels, relations?
        \item \texttt{can-jump(creek)} $\rightarrow$ What are these? 
        \item How does having these affordances affect your intention
            generation process?
        \item Need more examples...
    \end{itemize} 
\end{frame} 

\begin{frame}{Example} 
    \begin{center} 
        \includegraphics[width=0.35\textwidth]{example-flowchart} 
    \end{center} 
\end{frame} 


\end{document}
